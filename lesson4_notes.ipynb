{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson4-notes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brycexxx/fastai/blob/master/lesson4_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PAoNEDQz832l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson4\n",
        "\n",
        "### 1. NLP 中的迁移学习\n",
        "\n",
        "* 流程\n",
        "\n",
        "  <img src=\"https://raw.githubusercontent.com/Brycexxx/Images/master/20190209202212.jpg\"/>\n",
        "\n",
        "* language model：简言之，就是可以预测一个句子中下一个单词是什么的模型\n",
        "\n",
        "* 在对影评的分类中，难的是模型如何理解说英语，而不是判断这个人喜不喜欢这个电影。所以，首先通过维基百科大量的文档学习一个语言模型充分理解英语世界，然后通过 fine-tuning 针对影评学习一个语言模型，理解影评里句子的前后衔接。前两个阶段的语言模型的学习是一种自我监督学习模式，我们没有显示地为其创建标签，标签存在于文档数据集自身之中。\n",
        "\n",
        "* 语言模型对错误拼写的单词、俚语和缩写同样有效\n",
        "\n",
        "  为什么？（lesson4，12:47)\n",
        "\n",
        "  (Yes, absolutely it does. Particularly if you start with your wikitext model and then fine-tune it with your \"target\" corpus. Corpus is just a bunch of documents (emails, tweets, medical reports, or whatever). You could fine-tune it so it can learn a bit about the specifics of the slang , abbreviations, or whatever that didn't appear in the full corpus. So interestingly, this is one of the big things that people were surprised about when we did this research last year. People thought that learning from something like Wikipedia wouldn't be that helpful because it's not that representative of how people tend to write. But it turns out it's extremely helpful because there's a much a difference between Wikipedia and random words than there is between like Wikipedia and reddit. So it kind of gets you 99% of the way there.)\n",
        "\n",
        "* 对于 `kaggle` 提供的测试集，在训练语言模型的时候，可以将其和训练集合并，因为语言模型的标签就是数据集本身，所以这样合并我们可以得到一个更大的训练集，然后在这个更大的训练集中取出一部分作为验证集\n",
        "\n",
        "* 对于文本中存在大量的 emoji 该怎么训练？在维基百科中不会大量使用 emoji ，通常是一个 emoji 的词条，对应着相应的介绍。但是我们可以并且应该使用人们按照一般方式使用 emoji 的文本语料库进行微调（fine-tuning）。事实上，emoji 的使用从整体上来看，数量并不大，所以学习这些 emoji 是如何被使用的将会非常快\n",
        "\n",
        "### 2. Tabular data\n",
        "\n",
        "- 在处理缺失数据时，如果对某一列数据进行填补，则可以在数据中新增加一列，用来表示这一列的数据缺失与否\n",
        "- 通过 embedding 处理类别型数据\n",
        "\n",
        "### 3. 协同过滤\n",
        "\n",
        "- 冷启动问题，事实上在生产环境中使用协同过滤更加棘手，存在冷启动问题。当你有新用户时，你特别希望擅长推荐电影；当遇到新电影时，你特别在乎推荐的电影。但这个时候你的协同过滤系统中还没有任何相关数据，所以这很难\n",
        "- 冷启动的解决：实际上还只是停留在概念上，我们需要建立第二个模型，但这个模型不是协同过滤系统，而是一个元数据驱动的针对新用户新电影的模型\n",
        "- Netflix 解决冷启动问题的方式是：对于新注册的用户，系统会询问你对 20 部常见的电影的好恶，然后利用用户的回复再展示 20 部电影获取你的反馈，这个时候系统对新用户就不再有冷启动问题了；对于新电影冷启动问题就更好解决了，直接询问没看过这个新电影的用户是否喜欢这个电影就可以了\n",
        "- 当然有时候是不适合给用户太多的问题选择的，你只想给他你想让他们购买的。这个时候你可以尝试着使用基于 tabular 模型的元数据，根据他们来自哪里，年龄和性别等，做出一些初始推荐的猜测\n",
        "\n",
        "### 4. 小结\n",
        "\n",
        "通常最后一层不会使用 Relu 激活函数，我们想要的往往不会截止于 0，或者与无限大；通常使用 sigmoid 或者类似的，因为我们想要的结果往往在两个数值之间，我们可以通过 sigmoid 然后进行缩放。"
      ]
    }
  ]
}